{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebca637-682d-47c8-aac9-5406d5e3515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json  # only needed if save_summary_json=True\n",
    "\n",
    "def analyze_and_export_mistakes(\n",
    "    pred_csv_path: str,\n",
    "    out_mistakes_csv_path: str,\n",
    "    charts_dir: str | None = None,\n",
    "    truth_col: str = \"class_label\",\n",
    "    pred_col: str = \"predicted_label\",\n",
    "    id_col: str = \"tweet_id\",\n",
    "    text_col: str = \"tweet_text\",\n",
    "    save_summary_json: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pandas-first evaluation:\n",
    "      - loads predictions CSV\n",
    "      - exports misclassified rows\n",
    "      - computes accuracy, macro-F1, per-class metrics using crosstab\n",
    "      - saves charts and CSVs\n",
    "    Returns: (mistakes_df, summary_dict, per_class_df, conf_mat_df)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(pred_csv_path)\n",
    "    for c in (truth_col, pred_col):\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Column '{c}' not found in {pred_csv_path}\")\n",
    "\n",
    "    # restrict to rows with truth labels present\n",
    "    df_eval = df[df[truth_col].astype(str).str.len() > 0].copy()\n",
    "\n",
    "    # mistakes\n",
    "    mistakes_df = df_eval.loc[df_eval[truth_col] != df_eval[pred_col]].copy()\n",
    "    out_p = pathlib.Path(out_mistakes_csv_path)\n",
    "    out_p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    mistakes_df.to_csv(out_p, index=False)\n",
    "\n",
    "    # confusion matrix (rows=true, cols=pred)\n",
    "    labels = sorted(set(df_eval[truth_col]) | set(df_eval[pred_col]))\n",
    "    conf_mat_df = pd.crosstab(\n",
    "        df_eval[truth_col],\n",
    "        df_eval[pred_col],\n",
    "        dropna=False\n",
    "    ).reindex(index=labels, columns=labels, fill_value=0)\n",
    "\n",
    "    # per-class metrics (vectorized)\n",
    "    tp = np.diag(conf_mat_df.values)\n",
    "    support_true = conf_mat_df.sum(axis=1).values\n",
    "    support_pred = conf_mat_df.sum(axis=0).values\n",
    "    fp = support_pred - tp\n",
    "    fn = support_true - tp\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) != 0)\n",
    "    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) != 0)\n",
    "    f1        = np.divide(2*precision*recall, precision+recall,\n",
    "                          out=np.zeros_like(tp, dtype=float), where=(precision+recall) != 0)\n",
    "    error_rate = np.divide(fn + fp, support_true + fp, out=np.zeros_like(tp, dtype=float),\n",
    "                           where=(support_true + fp) != 0)\n",
    "\n",
    "    per_class_df = pd.DataFrame({\n",
    "        \"label\": labels,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"support\": support_true.astype(int),\n",
    "        \"error_rate\": error_rate\n",
    "    }).sort_values(\"label\")\n",
    "\n",
    "    accuracy = (tp.sum() / conf_mat_df.values.sum()) if conf_mat_df.values.sum() else 0.0\n",
    "    macro_f1 = float(per_class_df[\"f1\"].mean()) if not per_class_df.empty else 0.0\n",
    "\n",
    "    summary = {\n",
    "        \"num_total_with_truth\": int(len(df_eval)),\n",
    "        \"num_correct\": int(tp.sum()),\n",
    "        \"num_incorrect\": int(len(mistakes_df)),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    # charts & tables\n",
    "    if charts_dir:\n",
    "        charts_dir = pathlib.Path(charts_dir)\n",
    "        charts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # confusion matrix (counts)\n",
    "        plt.figure(figsize=(8 + 0.3*len(labels), 6 + 0.3*len(labels)))\n",
    "        plt.imshow(conf_mat_df.values, interpolation=\"nearest\")\n",
    "        plt.title(\"Confusion Matrix (counts)\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "        plt.yticks(range(len(labels)), labels)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(charts_dir / \"confusion_matrix.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # per-class F1\n",
    "        plt.figure(figsize=(max(8, 0.6*len(labels)), 5))\n",
    "        plt.bar(per_class_df[\"label\"], per_class_df[\"f1\"])\n",
    "        plt.title(\"Per-class F1\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(charts_dir / \"per_class_f1.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # per-class error rate\n",
    "        plt.figure(figsize=(max(8, 0.6*len(labels)), 5))\n",
    "        plt.bar(per_class_df[\"label\"], per_class_df[\"error_rate\"])\n",
    "        plt.title(\"Per-class Error Rate\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(charts_dir / \"per_class_error_rate.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # top confusions (off-diagonal)\n",
    "        C = conf_mat_df.values\n",
    "        pairs = [(labels[i], labels[j], int(C[i, j])) for i in range(len(labels)) for j in range(len(labels)) if i != j and C[i, j] > 0]\n",
    "        pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        top_k = pairs[:15]\n",
    "        if top_k:\n",
    "            plt.figure(figsize=(10, max(4, 0.4*len(top_k))))\n",
    "            ylabels = [f\"{t} â†’ {p}\" for t, p, _ in top_k]\n",
    "            counts = [c for _, _, c in top_k]\n",
    "            y = np.arange(len(top_k))\n",
    "            plt.barh(y, counts)\n",
    "            plt.yticks(y, ylabels)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.title(\"Top Confusions (off-diagonal)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(charts_dir / \"top_confusions.png\", dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "        # save numeric summaries\n",
    "        per_class_df.to_csv(charts_dir / \"per_class_metrics.csv\", index=False)\n",
    "        conf_mat_df.to_csv(charts_dir / \"confusion_matrix.csv\")\n",
    "        if save_summary_json:\n",
    "            with open(charts_dir / \"summary.json\", \"w\") as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "\n",
    "    return mistakes_df, summary, per_class_df, conf_mat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e749eefa-6f46-454a-810b-49c71d13c2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'num_total_with_truth': 752,\n",
       "  'num_correct': 529,\n",
       "  'num_incorrect': 223,\n",
       "  'accuracy': np.float64(0.7034574468085106),\n",
       "  'macro_f1': 0.590928846013498,\n",
       "  'labels': ['caution_and_advice',\n",
       "   'displaced_people_and_evacuations',\n",
       "   'infrastructure_and_utility_damage',\n",
       "   'injured_or_dead_people',\n",
       "   'missing_or_found_people',\n",
       "   'not_humanitarian',\n",
       "   'other_relevant_information',\n",
       "   'requests_or_urgent_needs',\n",
       "   'rescue_volunteering_or_donation_effort',\n",
       "   'sympathy_and_support']},\n",
       "                tweet_id                                         tweet_text  \\\n",
       " 3   1062711111869333504  BBC News - California wildfires: Nine dead and...   \n",
       " 10  1064454370731982848  @lucydragonn We are cooperating with the Count...   \n",
       " 11  1064237337775767552  Wondering what steps you can take to make Cali...   \n",
       " 12  1065590550726885379  Due to substrate Southern California (Malibu) ...   \n",
       " 13  1062489445172166656  Nearly 9,000 firefighters battling the #Califo...   \n",
       " \n",
       "                                class_label                   predicted_label  \\\n",
       " 3                   injured_or_dead_people  displaced_people_and_evacuations   \n",
       " 10  rescue_volunteering_or_donation_effort  displaced_people_and_evacuations   \n",
       " 11                        not_humanitarian                caution_and_advice   \n",
       " 12              other_relevant_information                caution_and_advice   \n",
       " 13  rescue_volunteering_or_donation_effort            injured_or_dead_people   \n",
       " \n",
       "     confidence  entropy  \n",
       " 3         0.90      NaN  \n",
       " 10        0.90      NaN  \n",
       " 11        0.85      NaN  \n",
       " 12        0.95      NaN  \n",
       " 13        0.95      NaN  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_csv = \"runs/california_wildfires_2018/dev/gpt-4o-mini/20251017-192014-modeS/predictions.csv\"\n",
    "\n",
    "mistakes_df, summary, per_cls, conf_df = analyze_and_export_mistakes(\n",
    "    pred_csv_path=pred_csv,\n",
    "    out_mistakes_csv_path=\"analysis/california_wildfires_2018/dev/gpt-4o-mini/mistakes.csv\",\n",
    "    charts_dir=\"analysis/california_wildfires_2018/dev/gpt-4o-mini/charts\",\n",
    "    truth_col=\"class_label\",          # adjust if your CSV uses different names\n",
    "    pred_col=\"predicted_label\",\n",
    "    id_col=\"tweet_id\",\n",
    "    text_col=\"tweet_text\",\n",
    ")\n",
    "\n",
    "summary, mistakes_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f20ec-f7db-4c1c-bc85-d5e6b05d44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
