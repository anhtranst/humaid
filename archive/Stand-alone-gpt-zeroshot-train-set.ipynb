{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d77fca-8b82-4d7d-8484-76bdaffc687b",
   "metadata": {},
   "source": [
    "# Setup (imports, config, labels, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6eb4a63-aab9-4072-a7c5-41200057f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, pathlib, datetime, requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Put your key in the environment once per runtime:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "load_dotenv()  # Automatically loads from `.env` in current dir\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in your environment.\"\n",
    "\n",
    "OPENAI_BASE = \"https://api.openai.com/v1\"\n",
    "H_JSON = {\"Authorization\": f\"Bearer {OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "H_MULTI = {\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"}\n",
    "\n",
    "# === Your label set (enum names) ===\n",
    "LABELS = [\n",
    "    \"caution_and_advice\",\n",
    "    \"displaced_people_and_evacuations\",\n",
    "    \"infrastructure_and_utility_damage\",\n",
    "    \"injured_or_dead_people\",\n",
    "    \"missing_or_found_people\",\n",
    "    \"not_humanitarian\",\n",
    "    \"other_relevant_information\",\n",
    "    \"requests_or_urgent_needs\",\n",
    "    \"rescue_volunteering_or_donation_effort\",\n",
    "    \"sympathy_and_support\",\n",
    "]\n",
    "\n",
    "# === Prompt pieces (short, deterministic) ===\n",
    "SYSTEM_PROMPT = (\n",
    "  \"You are a precise tweet classifier for humanitarian-response content. \"\n",
    "  \"Choose exactly one label from the allowed list that best fits the tweet. \"\n",
    "  \"If unrelated to humanitarian contexts, choose 'not_humanitarian'. \"\n",
    "  \"Follow the short rules and output JSON that matches the schema; no extra fields.\"\n",
    ")\n",
    "\n",
    "RULES_1 = (\n",
    "  \"- requests_or_urgent_needs: asking for help/supplies/SOS\\n\"\n",
    "  \"- rescue_volunteering_or_donation_effort: offering help, donation, organizing aid\\n\"\n",
    "  \"- caution_and_advice: warnings/instructions/tips\\n\"\n",
    "  \"- displaced_people_and_evacuations: evacuations, relocation, shelters\\n\"\n",
    "  \"- injured_or_dead_people: injuries, casualties, fatalities\\n\"\n",
    "  \"- missing_or_found_people: missing or found persons\\n\"\n",
    "  \"- infrastructure_and_utility_damage: damage/outages to roads/bridges/power/water/buildings\\n\"\n",
    "  \"- sympathy_and_support: prayers/condolences, no actionable info\\n\"\n",
    "  \"- other_relevant_information: on-topic but none of the above\\n\"\n",
    "  \"- not_humanitarian: unrelated to disasters/aid\\n\"\n",
    "  \"Tie-break: prefer actionable class when in doubt.\"\n",
    ")\n",
    "\n",
    "RULES_2 = (\n",
    "  \"- requests_or_urgent_needs: ASKING for help/supplies/services (need/please help/send/urgent/SOS). If both ask and offer words appear, ASKING wins.\\n\"\n",
    "  \"- rescue_volunteering_or_donation_effort: OFFERING help, organizing rescues, donation drives, fundraisers, volunteering sign-ups.\\n\"\n",
    "  \"- caution_and_advice: Warnings, instructions, actionable tips (evacuate/avoid/boil water). If only prayers/solidarity words, do NOT use this.\\n\"\n",
    "  \"- displaced_people_and_evacuations: Evacuation orders, relocations, sheltering, families displaced.\\n\"\n",
    "  \"- injured_or_dead_people: Injuries, casualties, fatalities.\\n\"\n",
    "  \"- missing_or_found_people: People reported missing OR confirmed found/located/reunited. If not explicit, do NOT use this.\\n\"\n",
    "  \"- infrastructure_and_utility_damage: Physical damage or outages to roads, bridges, buildings, power, water, comms, caused by the disaster. If disaster context is unclear, prefer not_humanitarian or other_relevant_information.\\n\"\n",
    "  \"- sympathy_and_support: Prayers, thoughts, condolences, “stay strong”, morale support ONLY (no requests, offers, warnings).\\n\"\n",
    "  \"- other_relevant_information: On-topic situation info that fits none of the above (e.g., event stats, forecasts, timelines) AND is clearly disaster-related.\\n\"\n",
    "  \"- not_humanitarian: Unrelated to disasters/aid or unclear/no disaster context.\\n\"\n",
    "  \"Tie-breakers:\\n\"\n",
    "  \"1) ASKING vs OFFERING → ASKING wins (requests_or_urgent_needs).\\n\"\n",
    "  \"2) People vs infrastructure → if injuries/casualties/missing are present, choose the people class.\\n\"\n",
    "  \"3) Sympathy vs caution → only actionable verbs → caution_and_advice; otherwise sympathy_and_support.\\n\"\n",
    "  \"4) Infra damage needs disaster context; otherwise not_humanitarian.\"\n",
    ")\n",
    "\n",
    "# === Structured Outputs schema for Mode S ===\n",
    "SCHEMA_S = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"label\": {\"type\": \"string\", \"enum\": LABELS},\n",
    "        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n",
    "    },\n",
    "    \"required\": [\"label\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93cd36-a764-4a5b-884a-94cc00e288a2",
   "metadata": {},
   "source": [
    "# Helpers: paths, TSV loader, tiny F1 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f332c81-1e7a-4c7d-8462-8ac059a883db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(path, id_col=\"tweet_id\", text_col=\"tweet_text\", label_col=\"class_label\"):\n",
    "    \"\"\"\n",
    "    Reads a TSV to a DataFrame and normalizes column names to tweet_id, tweet_text, class_label (optional).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\", dtype=str).fillna(\"\")\n",
    "    rename = {}\n",
    "    if id_col != \"tweet_id\":   rename[id_col] = \"tweet_id\"\n",
    "    if text_col != \"tweet_text\": rename[text_col] = \"tweet_text\"\n",
    "    if label_col and label_col in df.columns and label_col != \"class_label\":\n",
    "        rename[label_col] = \"class_label\"\n",
    "    df = df.rename(columns=rename)\n",
    "    assert \"tweet_id\" in df.columns and \"tweet_text\" in df.columns, \"Need tweet_id + tweet_text\"\n",
    "    if \"class_label\" not in df.columns:\n",
    "        df[\"class_label\"] = \"\"\n",
    "    return df\n",
    "\n",
    "def plan_run_dirs(\n",
    "    dataset_path: str,\n",
    "    out_root: str = \"runs\",\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    tag: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a tidy output layout:\n",
    "      runs/<event>/<split>/<model>/<run_id>/\n",
    "        - requests.jsonl\n",
    "        - outputs.jsonl\n",
    "        - predictions.csv\n",
    "    event = parent folder name (e.g., 'california_wildfires_2018')\n",
    "    split = file stem minus event (e.g., 'train' from 'california_wildfires_2018_train.tsv')\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(dataset_path)\n",
    "    event = p.parent.name\n",
    "    stem = p.stem\n",
    "    split = stem.replace(event, \"\").strip(\"_\") or \"data\"\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_id = f\"{ts}{('-'+tag) if tag else ''}\"\n",
    "    run_dir = pathlib.Path(out_root) / event / split / model / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\n",
    "        \"event\": event,\n",
    "        \"split\": split,\n",
    "        \"model\": model,\n",
    "        \"run_id\": run_id,\n",
    "        \"dir\": run_dir,\n",
    "        \"requests_jsonl\": run_dir / \"requests.jsonl\",\n",
    "        \"outputs_jsonl\": run_dir / \"outputs.jsonl\",\n",
    "        \"predictions_csv\": run_dir / \"predictions.csv\",\n",
    "        \"batch_meta_json\": run_dir / \"batch_meta.json\",\n",
    "    }\n",
    "\n",
    "def macro_f1(df, truth_col=\"class_label\", pred_col=\"predicted_label\"):\n",
    "    \"\"\"Quick macro-F1 without scikit; ignores rows with empty truth.\"\"\"\n",
    "    sub = df[(df[truth_col] != \"\") & (df[pred_col] != \"\")]\n",
    "    if sub.empty:\n",
    "        return float(\"nan\")\n",
    "    labels = sorted(set(sub[truth_col]) | set(sub[pred_col]))\n",
    "    f1s = []\n",
    "    for c in labels:\n",
    "        tp = ((sub[truth_col] == c) & (sub[pred_col] == c)).sum()\n",
    "        fp = ((sub[truth_col] != c) & (sub[pred_col] == c)).sum()\n",
    "        fn = ((sub[truth_col] == c) & (sub[pred_col] != c)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1   = 2*prec*rec / (prec+rec) if (prec+rec) else 0.0\n",
    "        f1s.append(f1)\n",
    "    return sum(f1s)/len(f1s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcd02f-a11d-494d-abad-91bd82bab292",
   "metadata": {},
   "source": [
    "# Dry-run (sync) on a small sample (no Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51d7ecd8-d644-4cbc-be2a-5f9d07316888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper: build the user message (ZERO-SHOT, no examples) ===\n",
    "def make_user_message(tweet_text: str, rules: str, labels: list[str]) -> str:\n",
    "    return (\n",
    "        f\"Allowed labels: {labels}\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        f\"{rules.strip()}\\n\"\n",
    "        \"Choose exactly one label. If unrelated, choose 'not_humanitarian'.\\n\"\n",
    "        f'Tweet: \"\"\"{tweet_text}\"\"\"'\n",
    "    )\n",
    "    \n",
    "# === Dry-run (sync) on a small sample ===\n",
    "def sync_test_sample(\n",
    "    df: pd.DataFrame,\n",
    "    n: int = 5,\n",
    "    rules: str = RULES_1,         # pass RULES_2 to compare\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    seed: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ZERO-SHOT test on n examples to verify output format & quick quality.\n",
    "    Returns a DataFrame with predicted_label, confidence (entropy=NaN in Mode S).\n",
    "    \"\"\"\n",
    "    test = df.sample(min(n, len(df)), random_state=seed).copy()\n",
    "    rows = []\n",
    "    for _, r in test.iterrows():\n",
    "        user_msg = make_user_message(str(r[\"tweet_text\"]), rules, LABELS)\n",
    "        body = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 1,\n",
    "            \"max_tokens\": 40,   # chat.completions uses max_tokens\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\"name\": \"tweet_label\", \"schema\": SCHEMA_S},\n",
    "            },\n",
    "        }\n",
    "        resp = requests.post(f\"{OPENAI_BASE}/chat/completions\", headers=H_JSON, json=body, timeout=60)\n",
    "        if resp.status_code != 200:\n",
    "            print(\">>> API error body:\", resp.text)\n",
    "        resp.raise_for_status()\n",
    "        choice = resp.json()[\"choices\"][0][\"message\"]\n",
    "        parsed = choice.get(\"parsed\")\n",
    "        if not parsed:\n",
    "            # chat.completions often returns JSON as a STRING in message.content\n",
    "            content = choice.get(\"content\", \"\")\n",
    "            parsed = json.loads(content) if content else {}\n",
    "        rows.append({\n",
    "            \"tweet_id\": r[\"tweet_id\"],\n",
    "            \"tweet_text\": r[\"tweet_text\"],\n",
    "            \"class_label\": r.get(\"class_label\", \"\"),\n",
    "            \"predicted_label\": parsed.get(\"label\", \"\"),\n",
    "            \"confidence\": parsed.get(\"confidence\", None),\n",
    "            \"entropy\": float(\"nan\"),  # Mode S has no distribution → no entropy\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    print(\"Macro-F1 (tiny sample):\", macro_f1(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c10d1-c889-4a81-8d0e-1490bde49258",
   "metadata": {},
   "source": [
    "# Build Batch requests.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fab6808-522d-45e1-be72-d3222ad97239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Batch requests.jsonl (Mode S, ZERO-SHOT) ===\n",
    "def build_requests_jsonl_S(\n",
    "    df: pd.DataFrame,\n",
    "    out_path: str,\n",
    "    rules: str = RULES_1,         # pass RULES_2 to compare\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a Batch-ready JSONL file targeting /v1/chat/completions with Structured Outputs.\n",
    "    ZERO-SHOT: only rules are included (no examples).\n",
    "    \"\"\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            tid = str(row[\"tweet_id\"]).strip()\n",
    "            text = str(row[\"tweet_text\"] or \"\").replace(\"\\r\", \" \").strip()\n",
    "            user_msg = make_user_message(text, rules, LABELS)\n",
    "            body = {\n",
    "                \"model\": model,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 1,\n",
    "                \"max_tokens\": 40,   # chat.completions uses max_tokens\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                ],\n",
    "                \"response_format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": {\"name\": \"tweet_label\", \"schema\": SCHEMA_S},\n",
    "                },\n",
    "            }\n",
    "            line = {\n",
    "                \"custom_id\": f\"tweet-{tid}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": body,\n",
    "            }\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908dcc5-1363-488b-ba35-7f03657658fa",
   "metadata": {},
   "source": [
    "# Batch helpers (upload → create → poll → download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed2671b1-18cb-4a64-b0b1-5648b1fa04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_for_batch(filepath: str) -> str:\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        r = requests.post(f\"{OPENAI_BASE}/files\", headers=H_MULTI,\n",
    "                          files={\"file\": (os.path.basename(filepath), f)},\n",
    "                          data={\"purpose\":\"batch\"}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"id\"]\n",
    "\n",
    "def create_batch(input_file_id: str, endpoint=\"/v1/chat/completions\", completion_window=\"24h\") -> str:\n",
    "    payload = {\"input_file_id\": input_file_id, \"endpoint\": endpoint, \"completion_window\": completion_window}\n",
    "    r = requests.post(f\"{OPENAI_BASE}/batches\", headers=H_JSON, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"id\"]\n",
    "\n",
    "def get_batch(batch_id: str) -> dict:\n",
    "    r = requests.get(f\"{OPENAI_BASE}/batches/{batch_id}\", headers=H_JSON, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def wait_for_batch(batch_id: str, poll_secs=15) -> dict:\n",
    "    while True:\n",
    "        info = get_batch(batch_id)\n",
    "        status = info.get(\"status\")\n",
    "        print(f\"[batch {batch_id}] status = {status}\")\n",
    "        if status in {\"completed\",\"failed\",\"cancelled\"}:\n",
    "            return info\n",
    "        time.sleep(poll_secs)\n",
    "\n",
    "def download_file_content(file_id: str, out_path: str) -> str:\n",
    "    r = requests.get(f\"{OPENAI_BASE}/files/{file_id}/content\", headers=H_JSON, timeout=300)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c8b30-c819-46bf-b6f4-1f0139ae7cf2",
   "metadata": {},
   "source": [
    "# Parse Batch outputs → DataFrame/CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42638b4b-d176-4285-b4ea-a9aee09e07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parse Batch outputs back to DataFrame (Mode S robust) ===\n",
    "def parse_outputs_S_to_df(outputs_jsonl_path: str, source_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Map id → local fields so we don’t pay tokens to echo text/labels\n",
    "    by_id = {\n",
    "        str(r[\"tweet_id\"]): {\n",
    "            \"tweet_text\": r[\"tweet_text\"],\n",
    "            \"class_label\": r.get(\"class_label\", \"\")\n",
    "        }\n",
    "        for _, r in source_df.iterrows()\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    with open(outputs_jsonl_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            tid = rec.get(\"custom_id\", \"\").replace(\"tweet-\", \"\")\n",
    "            choice = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"]\n",
    "            parsed = choice.get(\"parsed\")\n",
    "            if not parsed:\n",
    "                content = choice.get(\"content\", \"\")\n",
    "                if isinstance(content, list):  # very rare for chat.completions\n",
    "                    content = content[0].get(\"text\", \"\")\n",
    "                parsed = json.loads(content) if content else {}\n",
    "            local = by_id.get(tid, {\"tweet_text\": \"\", \"class_label\": \"\"})\n",
    "            rows.append({\n",
    "                \"tweet_id\": tid,\n",
    "                \"tweet_text\": local[\"tweet_text\"],\n",
    "                \"class_label\": local[\"class_label\"],\n",
    "                \"predicted_label\": parsed.get(\"label\", \"\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", None),\n",
    "                \"entropy\": float(\"nan\"),\n",
    "            })\n",
    "    return pd.DataFrame(rows, columns=[\"tweet_id\", \"tweet_text\", \"class_label\", \"predicted_label\", \"confidence\", \"entropy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab9166-fd53-4a18-897e-7b701c04ca88",
   "metadata": {},
   "source": [
    "# TEST WITH RULES 1 (end-to-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "505d77eb-a6c8-4f49-832c-b4b99559d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1 (on this tiny sample only): 0.6958333333333333\n",
      "Sample macro-F1: 0.6958333333333333\n"
     ]
    }
   ],
   "source": [
    "# 0) Choose an input TSV\n",
    "dataset_path = \"Dataset/HumAID/california_wildfires_2018/california_wildfires_2018_dev.tsv\"\n",
    "df = load_tsv(dataset_path, id_col=\"tweet_id\", text_col=\"tweet_text\", label_col=\"class_label\")\n",
    "\n",
    "# 1) Dry-run on 10 samples to sanity-check\n",
    "demo = sync_test_sample(df, n=20, rules=RULES_1, model=\"gpt-4o-mini\", temperature=0.0, seed=42)\n",
    "demo.head()\n",
    "print(\"Sample macro-F1:\", macro_f1(demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90119b83-ef6a-44f0-a0ff-ef06492bfd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote JSONL to: runs\\california_wildfires_2018\\dev\\gpt-4o-mini\\20251017-192014-modeS\\requests.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 2) Plan run dirs and build requests.jsonl\n",
    "plan = plan_run_dirs(dataset_path, out_root=\"runs\", model=\"gpt-4o-mini\", tag=\"modeS\")\n",
    "req_path = build_requests_jsonl_S(df, plan[\"requests_jsonl\"], model=plan[\"model\"], temperature=0.0)\n",
    "print(\"Wrote JSONL to:\", req_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56adac96-17f3-4489-ab46-9a2244b51be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = validating\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = validating\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = validating\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = validating\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = in_progress\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = finalizing\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = finalizing\n",
      "[batch batch_68f2f9c60e1c819094bb025dce1cc001] status = completed\n"
     ]
    }
   ],
   "source": [
    "# 3) Upload → Create batch → Poll\n",
    "file_id  = upload_file_for_batch(str(plan[\"requests_jsonl\"]))\n",
    "batch_id = create_batch(file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\")\n",
    "with open(plan[\"batch_meta_json\"], \"w\", encoding=\"utf-8\") as f: json.dump({\"file_id\":file_id,\"batch_id\":batch_id}, f, indent=2)\n",
    "info = wait_for_batch(batch_id, poll_secs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b7717c8-d3b6-4605-a454-10a088c9e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: runs\\california_wildfires_2018\\dev\\gpt-4o-mini\\20251017-192014-modeS\\predictions.csv\n",
      "Macro-F1: 0.590928846013498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1065844591805390849</td>\n",
       "      <td>Camp Fire leaves over 13,000 without homes thi...</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061320723749330944</td>\n",
       "      <td>So in a truly strange world, we have @RealJame...</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1063535672793944065</td>\n",
       "      <td>66 people have died and more than 600 are stil...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1062711111869333504</td>\n",
       "      <td>BBC News - California wildfires: Nine dead and...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1064807520802197504</td>\n",
       "      <td>Death toll in California’s #CampFire has climb...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                         tweet_text  \\\n",
       "0  1065844591805390849  Camp Fire leaves over 13,000 without homes thi...   \n",
       "1  1061320723749330944  So in a truly strange world, we have @RealJame...   \n",
       "2  1063535672793944065  66 people have died and more than 600 are stil...   \n",
       "3  1062711111869333504  BBC News - California wildfires: Nine dead and...   \n",
       "4  1064807520802197504  Death toll in California’s #CampFire has climb...   \n",
       "\n",
       "                        class_label                   predicted_label  \\\n",
       "0  displaced_people_and_evacuations  displaced_people_and_evacuations   \n",
       "1                  not_humanitarian                  not_humanitarian   \n",
       "2            injured_or_dead_people            injured_or_dead_people   \n",
       "3            injured_or_dead_people  displaced_people_and_evacuations   \n",
       "4            injured_or_dead_people            injured_or_dead_people   \n",
       "\n",
       "   confidence  entropy  \n",
       "0        0.90      NaN  \n",
       "1        0.90      NaN  \n",
       "2        0.95      NaN  \n",
       "3        0.90      NaN  \n",
       "4        0.90      NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Download outputs and parse\n",
    "out_file_id = info[\"output_file_id\"]\n",
    "download_file_content(out_file_id, str(plan[\"outputs_jsonl\"]))\n",
    "preds_df = parse_outputs_S_to_df(plan[\"outputs_jsonl\"], df)\n",
    "preds_df.to_csv(plan[\"predictions_csv\"], index=False)\n",
    "print(\"Saved predictions to:\", plan[\"predictions_csv\"])\n",
    "\n",
    "# 5) (Optional) Evaluate if truth labels exist\n",
    "print(\"Macro-F1:\", macro_f1(preds_df))\n",
    "preds_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50904a7-a942-44c6-aee7-16ebc60ec32a",
   "metadata": {},
   "source": [
    "# TEST WITH RULES 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bd0dab1-70db-4290-b61d-4d9d41774b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1 (tiny sample): 0.6592592592592593\n",
      "Sample macro-F1: 0.6592592592592593\n"
     ]
    }
   ],
   "source": [
    "# 0) Load TSV\n",
    "dataset_path = \"Dataset/HumAID/california_wildfires_2018/california_wildfires_2018_dev.tsv\"\n",
    "df = load_tsv(dataset_path, id_col=\"tweet_id\", text_col=\"tweet_text\", label_col=\"class_label\")\n",
    "\n",
    "# 1) Dry-run (choose RULES_1 or RULES_2)\n",
    "demo = sync_test_sample(df, n=20, rules=RULES_2, model=\"gpt-4o-mini\", temperature=0.0, seed=42)\n",
    "demo.head(); print(\"Sample macro-F1:\", macro_f1(demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094cd4df-4997-4649-822c-2859ee75ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote JSONL to: runs\\california_wildfires_2018\\dev\\gpt-4o-mini\\20251017-202955-modeS-RULES2\\requests.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 2) Plan run dirs (tag with the rules name so runs are distinct)\n",
    "plan = plan_run_dirs(dataset_path, out_root=\"runs\", model=\"gpt-4o-mini\", tag=\"modeS-RULES2\")\n",
    "\n",
    "# 3) Build requests.jsonl with the chosen rules (ZERO-SHOT)\n",
    "req_path = build_requests_jsonl_S(df, plan[\"requests_jsonl\"], rules=RULES_2, model=plan[\"model\"], temperature=0.0)\n",
    "\n",
    "print(\"Wrote JSONL to:\", req_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b83b11cb-cac4-4e89-b671-8379202030ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = validating\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = in_progress\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = finalizing\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = finalizing\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = finalizing\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = finalizing\n",
      "[batch batch_68f30a1f0e208190adf5b3debfc206b9] status = completed\n"
     ]
    }
   ],
   "source": [
    "# 4) Submit batch, wait, download, parse (same as before)\n",
    "file_id  = upload_file_for_batch(str(plan[\"requests_jsonl\"]))\n",
    "batch_id = create_batch(file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\")\n",
    "with open(plan[\"batch_meta_json\"], \"w\", encoding=\"utf-8\") as f: json.dump({\"file_id\":file_id,\"batch_id\":batch_id}, f, indent=2)\n",
    "info = wait_for_batch(batch_id, poll_secs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d2caa-d721-4a7f-a3f3-de5544c79e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0611ea4-6979-462f-8fa8-366c28d4c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: runs\\california_wildfires_2018\\dev\\gpt-4o-mini\\20251017-202955-modeS-RULES2\\predictions.csv\n",
      "Macro-F1: 0.6004880811940472\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1065844591805390849</td>\n",
       "      <td>Camp Fire leaves over 13,000 without homes thi...</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061320723749330944</td>\n",
       "      <td>So in a truly strange world, we have @RealJame...</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1063535672793944065</td>\n",
       "      <td>66 people have died and more than 600 are stil...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1062711111869333504</td>\n",
       "      <td>BBC News - California wildfires: Nine dead and...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1064807520802197504</td>\n",
       "      <td>Death toll in California’s #CampFire has climb...</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                         tweet_text  \\\n",
       "0  1065844591805390849  Camp Fire leaves over 13,000 without homes thi...   \n",
       "1  1061320723749330944  So in a truly strange world, we have @RealJame...   \n",
       "2  1063535672793944065  66 people have died and more than 600 are stil...   \n",
       "3  1062711111869333504  BBC News - California wildfires: Nine dead and...   \n",
       "4  1064807520802197504  Death toll in California’s #CampFire has climb...   \n",
       "\n",
       "                        class_label                   predicted_label  \\\n",
       "0  displaced_people_and_evacuations  displaced_people_and_evacuations   \n",
       "1                  not_humanitarian                  not_humanitarian   \n",
       "2            injured_or_dead_people            injured_or_dead_people   \n",
       "3            injured_or_dead_people  displaced_people_and_evacuations   \n",
       "4            injured_or_dead_people            injured_or_dead_people   \n",
       "\n",
       "   confidence  entropy  \n",
       "0        0.90      NaN  \n",
       "1        0.90      NaN  \n",
       "2        0.95      NaN  \n",
       "3        0.90      NaN  \n",
       "4        0.90      NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Download outputs and parse\n",
    "out_file_id = info[\"output_file_id\"]\n",
    "download_file_content(out_file_id, str(plan[\"outputs_jsonl\"]))\n",
    "preds_df = parse_outputs_S_to_df(plan[\"outputs_jsonl\"], df)\n",
    "preds_df.to_csv(plan[\"predictions_csv\"], index=False)\n",
    "print(\"Saved predictions to:\", plan[\"predictions_csv\"])\n",
    "\n",
    "# 6) (Optional) Evaluate if truth labels exist\n",
    "print(\"Macro-F1:\", macro_f1(preds_df))\n",
    "preds_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db07698-2faf-4776-88db-afaf37d9032d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
